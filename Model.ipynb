{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"sarcasmv3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>label</th>\n",
       "      <th>comment_token</th>\n",
       "      <th>parent_comment_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>nc and nh</td>\n",
       "      <td>politics</td>\n",
       "      <td>yeah i get that argument at this point i'd pre...</td>\n",
       "      <td>0</td>\n",
       "      <td>[nc, and, nh]</td>\n",
       "      <td>[yeah, i, get, that, argument, at, this, point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>you do know west teams play against west teams...</td>\n",
       "      <td>nba</td>\n",
       "      <td>the blazers and mavericks the wests 5 and 6 se...</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, do, know, west, teams, play, against, we...</td>\n",
       "      <td>[the, blazers, and, mavericks, the, wests, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creepeth</td>\n",
       "      <td>they were underdogs earlier today but since gr...</td>\n",
       "      <td>nfl</td>\n",
       "      <td>they're favored to win</td>\n",
       "      <td>0</td>\n",
       "      <td>[they, were, underdogs, earlier, today, but, s...</td>\n",
       "      <td>[they're, favored, to, win]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>icebrotha</td>\n",
       "      <td>this meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, meme, isn't, funny, none, of, the, \", n...</td>\n",
       "      <td>[deadass, don't, kill, my, buzz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cush2push</td>\n",
       "      <td>i could use one of those tools</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>yep can confirm i saw the tool they use for th...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, could, use, one, of, those, tools]</td>\n",
       "      <td>[yep, can, confirm, i, saw, the, tool, they, u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                            comment  \\\n",
       "0  Trumpbart                                         nc and nh    \n",
       "1  Shbshb906  you do know west teams play against west teams...   \n",
       "2   Creepeth  they were underdogs earlier today but since gr...   \n",
       "3  icebrotha  this meme isn't funny none of the \"new york ni...   \n",
       "4  cush2push                    i could use one of those tools    \n",
       "\n",
       "            subreddit                                     parent_comment  \\\n",
       "0            politics  yeah i get that argument at this point i'd pre...   \n",
       "1                 nba  the blazers and mavericks the wests 5 and 6 se...   \n",
       "2                 nfl                            they're favored to win    \n",
       "3  BlackPeopleTwitter                         deadass don't kill my buzz   \n",
       "4  MaddenUltimateTeam  yep can confirm i saw the tool they use for th...   \n",
       "\n",
       "   label                                      comment_token  \\\n",
       "0      0                                      [nc, and, nh]   \n",
       "1      0  [you, do, know, west, teams, play, against, we...   \n",
       "2      0  [they, were, underdogs, earlier, today, but, s...   \n",
       "3      0  [this, meme, isn't, funny, none, of, the, \", n...   \n",
       "4      0             [i, could, use, one, of, those, tools]   \n",
       "\n",
       "                                parent_comment_token  \n",
       "0  [yeah, i, get, that, argument, at, this, point...  \n",
       "1  [the, blazers, and, mavericks, the, wests, 5, ...  \n",
       "2                        [they're, favored, to, win]  \n",
       "3                   [deadass, don't, kill, my, buzz]  \n",
       "4  [yep, can, confirm, i, saw, the, tool, they, u...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def destroy(index):\n",
    "    data.drop(data.index[index], inplace = True)\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    all_words = [word for tokens in data[\"comment_token\"] for word in tokens]\n",
    "    sentence_lengths = [len(tokens) for tokens in data[\"comment_token\"]]\n",
    "    VOCAB = sorted(list(set(all_words)))\n",
    "    print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "    print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>label</th>\n",
       "      <th>comment_token</th>\n",
       "      <th>parent_comment_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145117</th>\n",
       "      <td>12poulet21</td>\n",
       "      <td>patch notes general gameplay fixed technology ...</td>\n",
       "      <td>gaming</td>\n",
       "      <td>no man's sky has a new update and its huge</td>\n",
       "      <td>0</td>\n",
       "      <td>[patch, notes, general, gameplay, fixed, techn...</td>\n",
       "      <td>[no, man's, sky, has, a, new, update, and, its...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                            comment  \\\n",
       "145117  12poulet21  patch notes general gameplay fixed technology ...   \n",
       "\n",
       "       subreddit                               parent_comment  label  \\\n",
       "145117    gaming  no man's sky has a new update and its huge       0   \n",
       "\n",
       "                                            comment_token  \\\n",
       "145117  [patch, notes, general, gameplay, fixed, techn...   \n",
       "\n",
       "                                     parent_comment_token  \n",
       "145117  [no, man's, sky, has, a, new, update, and, its...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.comment_token.map(len) == 1434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10717208 words total, with a vocabulary size of 168246\n",
      "Max sentence length is 1247\n"
     ]
    }
   ],
   "source": [
    "destroy(145117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>label</th>\n",
       "      <th>comment_token</th>\n",
       "      <th>parent_comment_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359549</th>\n",
       "      <td>ithinkPOOP</td>\n",
       "      <td>comcast comcast comcast comcast comcast comcas...</td>\n",
       "      <td>funny</td>\n",
       "      <td>10 months later and the swastika is still on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[comcast, comcast, comcast, comcast, comcast, ...</td>\n",
       "      <td>[10, months, later, and, the, swastika, is, st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                            comment  \\\n",
       "359549  ithinkPOOP  comcast comcast comcast comcast comcast comcas...   \n",
       "\n",
       "       subreddit                                     parent_comment  label  \\\n",
       "359549     funny  10 months later and the swastika is still on t...      0   \n",
       "\n",
       "                                            comment_token  \\\n",
       "359549  [comcast, comcast, comcast, comcast, comcast, ...   \n",
       "\n",
       "                                     parent_comment_token  \n",
       "359549  [10, months, later, and, the, swastika, is, st...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.comment_token.map(len) == 1247]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10715961 words total, with a vocabulary size of 168244\n",
      "Max sentence length is 284\n"
     ]
    }
   ],
   "source": [
    "destroy(359549)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10715961 words total, with a vocabulary size of 168244\n",
      "Max sentence length is 284\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in data[\"comment_token\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in data[\"comment_token\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24666928 words total, with a vocabulary size of 236138\n",
      "Max sentence length is 4281\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in data[\"parent_comment_token\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in data[\"parent_comment_token\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955615 words total, with a vocabulary size of 14300\n",
      "Max word length is 4281\n"
     ]
    }
   ],
   "source": [
    "all_words = data['subreddit']\n",
    "word_lengths = [len(tokens) for tokens in data[\"subreddit\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max word length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['subreddit','comment','parent_comment']]\n",
    "labels = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>nc and nh</td>\n",
       "      <td>yeah i get that argument at this point i'd pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nba</td>\n",
       "      <td>you do know west teams play against west teams...</td>\n",
       "      <td>the blazers and mavericks the wests 5 and 6 se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nfl</td>\n",
       "      <td>they were underdogs earlier today but since gr...</td>\n",
       "      <td>they're favored to win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>this meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>i could use one of those tools</td>\n",
       "      <td>yep can confirm i saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                            comment  \\\n",
       "0            politics                                         nc and nh    \n",
       "1                 nba  you do know west teams play against west teams...   \n",
       "2                 nfl  they were underdogs earlier today but since gr...   \n",
       "3  BlackPeopleTwitter  this meme isn't funny none of the \"new york ni...   \n",
       "4  MaddenUltimateTeam                    i could use one of those tools    \n",
       "\n",
       "                                      parent_comment  \n",
       "0  yeah i get that argument at this point i'd pre...  \n",
       "1  the blazers and mavericks the wests 5 and 6 se...  \n",
       "2                            they're favored to win   \n",
       "3                         deadass don't kill my buzz  \n",
       "4  yep can confirm i saw the tool they use for th...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,GlobalAveragePooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, Flatten, Dropout,MaxPooling1D\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "      features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comment</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>518197</th>\n",
       "      <td>pcmasterrace</td>\n",
       "      <td>killer app confirmed!</td>\n",
       "      <td>windows 10 comes with a 3d modeling program!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472026</th>\n",
       "      <td>PS4</td>\n",
       "      <td>yea its called playstationfourpointfive it wil...</td>\n",
       "      <td>is there any new version of ps4 coming?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580389</th>\n",
       "      <td>news</td>\n",
       "      <td>yes our government must take care of us withou...</td>\n",
       "      <td>to play devils advocate here we have no idea w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309623</th>\n",
       "      <td>wow</td>\n",
       "      <td>daily beatings are good for the character</td>\n",
       "      <td>how to help my girlfriend's character? so my g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108839</th>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>aurelion sol with stars?</td>\n",
       "      <td>jax with lamp posts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit                                            comment  \\\n",
       "518197     pcmasterrace                              killer app confirmed!   \n",
       "472026              PS4  yea its called playstationfourpointfive it wil...   \n",
       "580389             news  yes our government must take care of us withou...   \n",
       "309623              wow         daily beatings are good for the character    \n",
       "108839  leagueoflegends                           aurelion sol with stars?   \n",
       "\n",
       "                                           parent_comment  \n",
       "518197       windows 10 comes with a 3d modeling program!  \n",
       "472026            is there any new version of ps4 coming?  \n",
       "580389  to play devils advocate here we have no idea w...  \n",
       "309623  how to help my girlfriend's character? so my g...  \n",
       "108839                               jax with lamp posts   "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text for total corpus for tokenizing later\n",
    "data_text = x_train_text + x_test_text\n",
    "\n",
    "comment_top_words = 168000\n",
    "parent_comment_top_words = 236000\n",
    "subreddit_top_words = 14300\n",
    "\n",
    "comment_tokenizer = Tokenizer(num_words= comment_top_words)\n",
    "parent_tokenizer = Tokenizer(num_words = parent_comment_top_words)\n",
    "subreddit_tokenizer = Tokenizer(num_words = subreddit_top_words)\n",
    "\n",
    "comment_tokenizer.fit_on_texts(features['comment'])\n",
    "parent_tokenizer.fit_on_texts(features['parent_comment'])\n",
    "subreddit_tokenizer.fit_on_texts(features['subreddit'])\n",
    "\n",
    "comment_train_tokens = comment_tokenizer.texts_to_sequences(x_train_text['comment'])\n",
    "parent_train_tokens = parent_tokenizer.texts_to_sequences(x_train_text['parent_comment'])\n",
    "subreddit_train_tokens = subreddit_tokenizer.texts_to_sequences(x_train_text['subreddit'])\n",
    "\n",
    "test_comment_train_tokens = comment_tokenizer.texts_to_sequences(x_test_text['comment'])\n",
    "test_parent_train_tokens = parent_tokenizer.texts_to_sequences(x_test_text['parent_comment'])\n",
    "test_subreddit_train_tokens = subreddit_tokenizer.texts_to_sequences(x_test_text['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = comment_top_words + parent_comment_top_words + subreddit_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2344, 1167, 540]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_dict = comment_tokenizer.word_index\n",
    "inv_comment = {v: k for k, v in comment_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "killer app confirmed\n"
     ]
    }
   ],
   "source": [
    "print(inv_comment[2344],inv_comment[1167], inv_comment[540])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[854, 200, 424, 16, 3, 3002, 14201, 1319]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dict = parent_tokenizer.word_index\n",
    "inv_parent = {v: k for k, v in parent_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows 10 comes with a 3d modeling program\n"
     ]
    }
   ],
   "source": [
    "print(inv_parent[854],inv_parent[200],inv_parent[424],inv_parent[16],inv_parent[3],inv_parent[3002],inv_parent[14201],inv_parent[1319])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_dict = subreddit_tokenizer.word_index\n",
    "inv_subreddit = {v: k for k, v in subreddit_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pcmasterrace'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_subreddit[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(train, test):\n",
    "    num_tokens = [len(tokens) for tokens in train + test]\n",
    "    return num_tokens\n",
    "\n",
    "def max_tokens(token_list):\n",
    "    return np.mean(token_list) + 2.5 * np.std(token_list)\n",
    "\n",
    "num_comment_token = token_count(comment_train_tokens,test_comment_train_tokens)\n",
    "max_comment_token = max_tokens(num_comment_token)\n",
    "max_comment_token = int(max_comment_token)\n",
    "\n",
    "num_parent_token = token_count(parent_train_tokens,test_parent_train_tokens)\n",
    "max_parent_token = max_tokens(num_parent_token)\n",
    "max_parent_token = int(max_parent_token)\n",
    "\n",
    "max_subreddit_token = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = max_comment_token + max_parent_token + max_subreddit_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "pad = 'pre'\n",
    "\n",
    "comment_train_pad = pad_sequences(comment_train_tokens, maxlen = max_comment_token, padding=pad, truncating=pad)\n",
    "parent_train_pad = pad_sequences(parent_train_tokens, maxlen = max_parent_token, padding=pad, truncating=pad)\n",
    "reddit_train_pad = pad_sequences(subreddit_train_tokens, maxlen = max_subreddit_token, padding=pad, truncating=pad)\n",
    "\n",
    "comment_test_pad = pad_sequences(test_comment_train_tokens, maxlen=max_comment_token, padding=pad, truncating=pad)\n",
    "parent_test_pad = pad_sequences(test_parent_train_tokens, maxlen = max_parent_token, padding=pad, truncating=pad)\n",
    "reddit_test_pad = pad_sequences(test_subreddit_train_tokens, maxlen = max_subreddit_token, padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764492, 30)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764492, 129)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764492, 21)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.hstack([reddit_train_pad, comment_train_pad, parent_train_pad])\n",
    "test_x = np.hstack([reddit_test_pad, comment_test_pad, parent_test_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764492, 180)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191123, 180)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical  # Makes \"one-hot\" encoding from label\n",
    "\n",
    "y_train_hot = to_categorical(y_train, num_classes=2)\n",
    "y_test_hot = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_vocab:  418300\n",
      "total_token:  180\n"
     ]
    }
   ],
   "source": [
    "print('total_vocab: ', total_vocab)\n",
    "print('total_token: ', total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "764492/764492 [==============================] - 461s 603us/step - loss: 0.5625 - acc: 0.7058\n",
      "Epoch 2/10\n",
      "764492/764492 [==============================] - 456s 596us/step - loss: 0.4963 - acc: 0.7585\n",
      "Epoch 3/10\n",
      "764492/764492 [==============================] - 453s 593us/step - loss: 0.4278 - acc: 0.8030\n",
      "Epoch 4/10\n",
      "764492/764492 [==============================] - 453s 592us/step - loss: 0.3531 - acc: 0.8442\n",
      "Epoch 5/10\n",
      "764492/764492 [==============================] - 453s 592us/step - loss: 0.2847 - acc: 0.8782\n",
      "Epoch 6/10\n",
      "764492/764492 [==============================] - 454s 594us/step - loss: 0.2260 - acc: 0.9054\n",
      "Epoch 7/10\n",
      "764492/764492 [==============================] - 455s 595us/step - loss: 0.1817 - acc: 0.9248\n",
      "Epoch 8/10\n",
      "764492/764492 [==============================] - 453s 593us/step - loss: 0.1481 - acc: 0.9396\n",
      "Epoch 9/10\n",
      "764492/764492 [==============================] - 454s 594us/step - loss: 0.1248 - acc: 0.9497\n",
      "Epoch 10/10\n",
      "764492/764492 [==============================] - 455s 596us/step - loss: 0.1061 - acc: 0.9576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf80a5e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim= total_vocab,\n",
    "                    output_dim= 64,\n",
    "                    input_length = total_tokens))\n",
    "\n",
    "model.add(Convolution1D(256, 3, padding='same'))\n",
    "model.add(Convolution1D(128, 3, padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(64, 3, padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(32, 3, padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(16, 3, padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(8, 3, padding='same'))\n",
    "model.add(Convolution1D(8, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(180,activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, y_train_hot, epochs=10, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.29%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "scores = model.evaluate(test_x, y_test_hot, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
